FILEPATH<-"C:/Users/kja505/Documents/NetlogoTest/"
file.exists(FILEPATH)
FILEPATH<-"C:/Users/kja505/Documents/NetlogoTest"
file.exists(FILEPATH)
FILEPATH<-"C:/Users/kja505/Documents/NetlogoTest"
PARAMETERS<-c("people","infectiousness","chance-recover","duration")
NUMSAMPLES<-500
PARAMVALS<-c(150,"[10,90]","[10,90]","[5,40]")
NUMSAMPLES<-500
NETLOGO_SETUP_FUNCTION<-"setup"
NETLOGO_RUN_FUNCTION<-"go"
MEASURES<-c("death-thru-sickness","death-but-immune","death-old-age",
"death-old-and-sick")
EXPERIMENT_REPETITIONS<-1
RUNMETRICS_EVERYSTEP<-"true"
ALGORITHM<-"normal"
lhc_generate_lhc_sample_netlogo(FILEPATH,PARAMETERS,PARAMVALS,NUMSAMPLES,ALGORITHM,
EXPERIMENT_REPETITIONS,RUNMETRICS_EVERYSTEP,NETLOGO_SETUP_FUNCTION,
NETLOGO_RUN_FUNCTION,MEASURES)
library(spartan)
library(spartan)
devtools::build()
vignette(package=spartan)
vignette(package="spartan")
glm_fit <- generate_requested_emulations(c("GLM"),partitionedData, parameters,measures, normalised=TRUE)
devtools::load_all(".")
glm_fit <- generate_requested_emulations(c("GLM"),partitionedData, parameters,measures, normalised=TRUE)
parameters
parameters<-c("Ka","Ki","Kr","Rf","Koff","Kdes","DIFFUSION_COEFFICIENT_CXCL13",
"TRAVEL_DISTANCE","POLARITY","CXCL13_EMITTED_FDC",
"CXCL13_EMITTED_MRC","CXCL13_EMITTED_FRC", "DECAY_CONSTANT_CXCL13")
measure_scale<-c("microns/min","microns","microns","microns")
measures<-c("MC","MI","Speed","checkPointsReached")
head(dataset)
dataset <- read.csv(â€œC:/Users/kja505/Desktop/JC/LHC_Summary.csv")
dataset <- read.csv("C:/Users/kja505/Desktop/JC/LHC_Summary.csv")
partitionedData <- partition_dataset(dataset,percent_train=75, percent_test=15,
percent_validation=10, seed=NULL, normalise=TRUE, parameters, sampleMins,
sampleMaxes)
sampleMaxes <- cbind(9.8,0.0103,0.01,98000,0.0098,0.105,15.6,0.94,0.975,2,2,2,0.001)
sampleMins <- cbind(0.8,0.0013,0.001,8000,0.0008,0.005,1.6,0.34,0.075,0,0,0,0.001)
partitionedData <- partition_dataset(dataset,percent_train=75, percent_test=15,
percent_validation=10, seed=NULL, normalise=TRUE, parameters, sampleMins,
sampleMaxes)
networkStructures<-list(c(14,7,4),c(14,7,7,4),c(14,10,1))
algorithmSettings<-emulation_algorithm_settings(network_structures=networkStructures)
getwd()
setwd("C:/Users/kja505/Desktop/JC/")
glm_fit <- generate_requested_emulations(c("GLM"),partitionedData, parameters,measures, normalised=TRUE)
head(dataset)
parameters
measures
sampleMins
sampleMaxes
partitionedData
partitionedData$training#
sampleMins
sampleMaxes
sampleMaxes <- cbind(9.8,0.0103,0.01,98000,0.0098,0.105,15.6,0.94,0.975,2,2,2,0.05)
sampleMins <- cbind(0.8,0.0013,0.001,8000,0.0008,0.005,1.6,0.34,0.075,0,0,0,0.001)
glm_fit <- generate_requested_emulations(c("GLM"),partitionedData, parameters,measures, normalised=TRUE)
partitionedData <- partition_dataset(dataset,percent_train=75, percent_test=15,
percent_validation=10, seed=NULL, normalise=TRUE, parameters, sampleMins,
sampleMaxes)
glm_fit <- generate_requested_emulations(c("GLM"),partitionedData, parameters,measures, normalised=TRUE)
svm_fit <- generate_requested_emulations(c("SVM"),partitionedData, parameters,measures, normalised=TRUE)
rf_fit <- generate_requested_emulations(c("RF"),partitionedData, parameters,measures, normalised=TRUE)
algorithmSettings
algorithmSettings$num_trees
rf_fit <- generate_requested_emulations(c("RF"),partitionedData, parameters,measures, normalised=TRUE)
rf_fit <- generate_requested_emulations(c("RF"),partitionedData, parameters,measures, normalised=TRUE)
devtools::load_all("C:/Users/kja505/Downloads/spartan_devtools/spartan_devtools/spartan")
rf_fit <- generate_requested_emulations(c("RF"),partitionedData, parameters,measures, normalised=TRUE)
nn_fit <- generate_requested_emulations(c("NNET"),partitionedData, parameters,measures,algorithmSettings, normalised=TRUE)
nn_fit <- generate_requested_emulations(c("NNET"),partitionedData, parameters,measures,algorithmSettings, normalised=TRUE)
devtools::load_all("C:/Users/kja505/Downloads/spartan_devtools/spartan_devtools/spartan")
nn_fit <- generate_requested_emulations(c("NNET"),partitionedData, parameters,measures,algorithmSettings, normalised=TRUE)
networkStructures<-list(c(14,7,4))
nn_fit <- generate_requested_emulations(c("NNET"),partitionedData, parameters,measures,algorithmSettings, normalised=TRUE)
partitionedData <- partition_dataset(dataset,percent_train=15, percent_test=75,
percent_validation=10, seed=NULL, normalise=TRUE, parameters, sampleMins,
sampleMaxes)
algorithmSettings<-emulation_algorithm_settings(network_structures=networkStructures)
nn_fit <- generate_requested_emulations(c("NNET"),partitionedData, parameters,measures,algorithmSettings, normalised=TRUE)
networkStructures<-list(c(14,7,4),c(14,7,7,4),c(14,10,1))
algorithmSettings<-emulation_algorithm_settings(network_structures=networkStructures)
nn_fit <- generate_requested_emulations(c("NNET"),partitionedData, parameters,measures,algorithmSettings, normalised=TRUE)
gp_fit <- generate_requested_emulations(c("GP"),partitionedData, parameters,measures, normalised=TRUE)
emulator_predictions(gp_fit, parameters, measures,	partitionedData$validation, normalise=FALSE)
predictions <- emulator_predictions(gp_fit, parameters, measures,	partitionedData$validation, normalise=TRUE)
predictions <- emulator_predictions(gp_fit, parameters, measures,	partitionedData$validation, normalise=FALSE)
predictions <- emulator_predictions(gp_fit, parameters, measures,	partitionedData$validation, normalise=TRUE)
parameters
measures
head(partitionedData$validation)
partitionedData$pre_normed_mins
partitionedData$pre_normed_maxes
sampleMaxes <- cbind(9.8,0.0103,0.01,98000,0.0098,0.105,15.6,0.94,0.975,50,50,50,0.05)
sampleMins <- cbind(0.8,0.0013,0.001,8000,0.0008,0.005,1.6,0.34,0.075,0,0,0,0.001)
partitionedData <- partition_dataset(dataset,percent_train=15, percent_test=75,
percent_validation=10, seed=NULL, normalise=TRUE, parameters, sampleMins,
sampleMaxes)
glm_fit <- generate_requested_emulations(c("GLM"),partitionedData, parameters,measures, normalised=TRUE)
svm_fit <- generate_requested_emulations(c("SVM"),partitionedData, parameters,measures, normalised=TRUE)
rf_fit <- generate_requested_emulations(c("RF"),partitionedData, parameters,measures, normalised=TRUE)
nn_fit <- generate_requested_emulations(c("NNET"),partitionedData, parameters,measures,algorithmSettings, normalised=TRUE)
gp_fit <- generate_requested_emulations(c("GP"),partitionedData, parameters,measures, normalised=TRUE)
predictions <- emulator_predictions(gp_fit, parameters, measures,	partitionedData$validation, normalise=TRUE)
predictions <- emulator_predictions(gp_fit, parameters, measures,	partitionedData$validation, normalise=TRUE)
data_to_predict <- partitionedData$validation
emulation <- gp_fit
parameters
emulation$pre_normed_mins
emulation$pre_normed_mins[parameters]
data_to_predict[,parameters]
head(data_to_predict[,parameters])
data_to_predict <- normalise_dataset(data_to_predict,
emulation$pre_normed_mins[parameters],
emulation$pre_normed_maxes[parameters],
parameters)
data_to_predict<-partitioneData$validation
data_to_predict<-partitionedData$validation
predictions <- emulator_predictions(gp_fit, parameters, measures,	partitionedData$validation, normalise=TRUE)
data_to_predict <- normalise_dataset(data_to_predict,
emulation$pre_normed_mins[parameters],
emulation$pre_normed_maxes[parameters],
parameters)
devtools::load_all("C:/Users/kja505/Downloads/spartan_devtools/spartan_devtools/spartan")
predictions <- emulator_predictions(gp_fit, parameters, measures,	partitionedData$validation, normalise=TRUE)
predictions <- emulator_predictions(gp_fit, parameters, measures,	partitionedData$validation, normalise=FALSE)
devtools::load_all("C:/Users/kja505/Downloads/spartan_devtools/spartan_devtools/spartan")
partitionedData
dataset
predictions <- emulator_predictions(gp_fit, parameters, measures,	partitionedData$validation, normalise=TRUE)
predictions <- emulator_predictions(gp_fit, parameters, measures, dataset, normalise=TRUE)
head(dataset)
head(dataset[,parameters])
emulation<-gp_fit
data_to_predict<-dataset
data_to_predict <- normalise_dataset(data_to_predict,
emulation$pre_normed_mins[parameters],
emulation$pre_normed_maxes[parameters],
parameters)
head(data_to_predict)
nrow(dataset)
nrow(data_to_predict
)
data_to_predict<-dataset
nrow(data_to_predict)
data_to_predict <- normalise_dataset(data_to_predict,
emulation$pre_normed_mins[parameters],
emulation$pre_normed_maxes[parameters],
parameters)
nrow(data_to_predict)
nrow(data_to_predict$scaled)
head(data_to_predict$scaled)
devtools::load_all("C:/Users/kja505/Downloads/spartan_devtools/spartan_devtools/spartan")
data_to_predict<-dataset
predictions <- emulator_predictions(gp_fit, parameters, measures, dataset, normalise=TRUE)
head(predictions)
exising_simulations <- list(glm_fit,svm_fit,rf_fit,nn_fit,gp_fit)
generated_ensemble <- generate_ensemble_from_existing_emulations(existing_simulations,parameters, measures, partitionedData$testing,algorithmSettings=algorithmSettings, timepoint=NULL)
generated_ensemble <- generate_ensemble_from_existing_emulations(existing_simulations,parameters, measures, partitionedData$testing,algorithm_settings=algorithmSettings, timepoint=NULL)
generated_ensemble <- generate_ensemble_from_existing_emulations(existing_simulations,parameters, measures, partitionedData$testing,algorithm_settings=algorithmSettings, timepoint=NULL)
existing_simulations <- list(glm_fit,svm_fit,rf_fit,nn_fit,gp_fit)
generated_ensemble <- generate_ensemble_from_existing_emulations(existing_simulations,parameters, measures, partitionedData$testing,algorithm_settings=algorithmSettings, timepoint=NULL)
devtools::load_all("C:/Users/kja505/Downloads/spartan_devtools/spartan_devtools/spartan")
generated_ensemble <- generate_ensemble_from_existing_emulations(existing_simulations,parameters, measures, partitionedData$testing,algorithm_settings=algorithmSettings, normalised=FALSE, timepoint=NULL)
devtools::load_all("C:/Users/kja505/Downloads/spartan_devtools/spartan_devtools/spartan")
generated_ensemble <- generate_ensemble_from_existing_emulations(existing_simulations,parameters, measures, partitionedData$testing,algorithm_settings=algorithmSettings, normalised=FALSE, timepoint=NULL)
generated_ensemble <- generate_ensemble_from_existing_emulations(existing_simulations,parameters, measures, partitionedData$testing,algorithm_settings=algorithmSettings, normalise=TRUE, timepoint=NULL)
existing_emulations<-existing_simulations
observed_data = partitionedData$testing
algorithm_settings=algorithmSettings
normalise=TRUE
timepoint=NULL
start.time <- proc.time()
all_model_predictions <- NULL
emulator_types <- NULL
ensemble_emulators <- vector("list")
generation_time <- 0
pre_normed_mins <- NULL
pre_normed_maxes <- NULL
## As we may have a compound structure of emulators (depending on the call),
# we need to keep a reference that can combine each into one list
emulator_ref <- 1
# First we are going to examine all items in the existing_emulations list
for (model in 1:length(existing_emulations)) {
# Get the object of emulations that was returned when this was generated
# (from generate_requested_emulation)
emulator <- existing_emulations[[model]]
# This in turn may include more than one emulation (if the user used the
# multiple method instead of generating each separately, then attached
# several of these objects)
# So we need to check for this and cycle through these
if (length(emulator$emulators) > 0) {
for (sub_emulator in 1:length(emulator$emulators)) {
# Get this enclosed emulator
inlist_emulator <- emulator$emulators[[sub_emulator]]
emulator_types <- c(emulator_types,
emulator$emulators[[sub_emulator]]$type)
all_model_predictions <- generate_ensemble_training_set(
inlist_emulator, parameters, measures, observed_data,
all_model_predictions)
ensemble_emulators[[emulator_ref]] <- inlist_emulator
#  # Get the generation time and add this to the generation time
generation_time <- generation_time +
emulator$emulators[[sub_emulator]]$benchmark
# If the pre_normed_mins and maxes are still NULL, see if this emulator
# has set them. If it has, we make the assumption this is the case for
# them all (as they should have all been generated on the same scale
if (is.null(pre_normed_maxes) & is.null(pre_normed_mins)) {
pre_normed_maxes <-
emulator$emulators[[sub_emulator]]$pre_normed_maxes
pre_normed_mins <-
emulator$emulators[[sub_emulator]]$pre_normed_mins
}
emulator_ref <- emulator_ref + 1
}
} else  {
# Only one emulator in this object
emulator_types <- c(emulator_types, emulator$type)
all_model_predictions <- generate_ensemble_training_set(
emulator, parameters, measures, observed_data, all_model_predictions)
ensemble_emulators[[emulator_ref]] <- emulator
#  # Get the generation time and add this to the generation time
generation_time <- generation_time + emulator$benchmark
# If the pre_normed_mins and maxes are still NULL, see if this
# emulator has set them. If it has, we make the assumption this
# is the case for them all (as they should have all been
# generated on the same scale
if (is.null(pre_normed_maxes) & is.null(pre_normed_mins)) {
pre_normed_maxes <- emulator$pre_normed_maxes
pre_normed_mins <- emulator$pre_normed_mins
}
emulator_ref <- emulator_ref + 1
}
}
ensemble_emulations <- ensemble_emulators
all_emulator_predictions <- all_model_predictions
emulator_test_data<-observed_data
if (is.null(algorithm_settings))
algorithm_settings <- emulation_algorithm_settings()
# Calculate the weightings
weights <- calculate_weights_for_ensemble_model(
all_emulator_predictions, emulator_test_data, measures, emulator_types,
algorithm_settings$num_of_generations)
# Generate prediction from the ensemble
ensemble_predictions <- weight_emulator_predictions_by_ensemble(
weights, all_emulator_predictions, measures,
algorithm_settings$num_of_generations)
unscaled_predictions <- denormalise_dataset(
ensemble_predictions, rbind(ensemble_emulations$pre_normed_mins[measures]),
rbind(ensemble_emulations$pre_normed_maxes[measures]))
normalised_data<-ensemble_predictions
scaled_mins<-rbind(ensemble_emulations$pre_normed_mins[measures])
scaled_maxes<-rbind(ensemble_emulations$pre_normed_maxes[measures])
c<-1
ncol(normalised_data)
head(normalised_data)
scaled_maxes
ensemble_emulations
ensemble_emulations$p
devtools::load_all("C:/Users/kja505/Downloads/spartan_devtools/spartan_devtools/spartan")
generated_ensemble <- generate_ensemble_from_existing_emulations(existing_simulations,parameters, measures, partitionedData$testing,algorithm_settings=algorithmSettings, normalise=TRUE, timepoint=NULL)
generated_ensemble <- generate_ensemble_from_existing_emulations(existing_simulations,parameters, measures, partitionedData$testing,algorithm_settings=algorithmSettings, normalise=TRUE, timepoint=NULL)
Q
devtools::load_all("C:/Users/kja505/Downloads/spartan_devtools/spartan_devtools/spartan")
generated_ensemble <- generate_ensemble_from_existing_emulations(existing_simulations,parameters, measures, partitionedData$testing,algorithm_settings=algorithmSettings, normalise=TRUE, timepoint=NULL)
params<-partitionedData$validation
params<-partitionedData$validation
predictions <- use_ensemble_to_generate_predictions(generated_ensemble,params,
parameters,measures,normalise=TRUE)
predictions
numCurves <- 3
numCurves <- 3
numSamples <- 65
filepath <- getwd()
pmin <- sampleMins
pmax <- sampleMaxes
efast_generate_sample(filepath, numCurves,numSamples,parameters, pmin, pmax)
emulate_efast_sampled_parameters(filepath, ensemble, parameters, measures, numCurves)
emulate_efast_sampled_parameters(filepath, generated_ensemble, parameters, measures, numCurves)
efast_run_Analysis(filepath,measures,parameters,numCurves, numSamples,1:2,0.95,TRUE,"eFAST_Analysis.csv",NULL,NULL)
devtools::build()
setwd("C:\Users\kja505\Downloads\spartan_devtools\spartan_devtools")
setwd("C:/Users/kja505/Downloads/spartan_devtools/spartan_devtools")
devtools::build()
library(spartan)
devtools::build()
devtools::load_all(".")
vignettes()
vignette(package="spartan")
devtools::load_all(".")
devtools::build()
exports(package="spartan")
export(package="spartan")
devtools::load_all(".")
devtools::document()
devtools::build()
lhcResults <- read.csv("/home/kja505/Documents/LHC_Spartan2/LHC_AllResults.csv",header=T)
head(lhcResults)
devtools::use_data(lhcResults,spartan)
devtools::load_all(".")
devtools::use_data(lhcResults,spartan)
devtools::use_data(lhcResults,spartan,overwrite=TRUE)
devtools::use_data(lhcResults,overwrite=TRUE)
sim_data_for_emulation <- read.csv("/home/kja505/Dropbox/IEEE_AIS/Emulation_Training_Data/Spartan_Tutorial/LHCSummary_72.csv",header=T)
devtools::use_data(sim_data_for_emulation)
devtools::load_all(".")
data("sim_data_for_emulation")
rm(list=ls())
devtools::load_all(".")
data("sim_data_for_emulation")
head(sim_data_for_emulation)
getwd()
parameters<-c("stableBindProbability","chemokineExpressionThreshold","initialChemokineExpressionValue","maxChemokineExpressionValue","maxProbabilityOfAdhesion","adhesionFactorExpressionSlope")
# Output measures
measures<-c("Velocity","Displacement","PatchArea")
# Mins and max values used in sampling
sampleMaxes <- cbind(0,0.9,0.5,0.08,1,5)
sampleMins <-cbind(100,0.1,0.1,0.015,0.1,0.25)
## Partition the dataset, in this case normalising the data
partitionedData <- partition_dataset(dataset,percent_train=75, percent_test=15,
percent_validation=10, seed=NULL, normalise=TRUE, parameters, sampleMins,
sampleMaxes
)
partitionedData <- partition_dataset(sim_data_for_emulation,percent_train=75, percent_test=15,
percent_validation=10, seed=NULL, normalise=TRUE, parameters, sampleMins,
sampleMaxes)
visualise_data_distribution(partitionedData$training,"Velocity","Velocity_Diagnostic")
devtools::load_all(".")
visualise_data_distribution(partitionedData$training,"Velocity","Velocity_Diagnostic")
devtools::load_all(".")
visualise_data_distribution(partitionedData$training,"Velocity","Velocity_Diagnostic")
dataset<-partitionedData$training
measure<-"Velocity"
graphName<-"Test"
kurt <- psych::describe(dataset)
dataset <- data.frame(dataset)
ggplot(dataset,
aes(dataset)) + geom_histogram() + labs(
x = "Dataset", y = "Frequency") + ggtitle(
paste("Diagnostic Plot for ", measure, "\nKurtosis:",
round(kurt$kurtosis, 3), "Skew", round(kurt$skew, 3),
sep = " "))
library(ggplot2)
ggplot(dataset,
aes(dataset)) + geom_histogram() + labs(
x = "Dataset", y = "Frequency") + ggtitle(
paste("Diagnostic Plot for ", measure, "\nKurtosis:",
round(kurt$kurtosis, 3), "Skew", round(kurt$skew, 3),
sep = " "))
head(dataset)
measure
head(dataset[measure])
devtools::load_all(".")
visualise_data_distribution(partitionedData$training,"Velocity","Velocity_Diagnostic")
devtools::load_all(".")
visualise_data_distribution(partitionedData$training,"Velocity","Velocity_Diagnostic")
devtools::load_all(".")
visualise_data_distribution(partitionedData$training,"Velocity","Velocity_Diagnostic")
data("sim_data_for_emulation")
# Simulation parameters
parameters<-c("stableBindProbability","chemokineExpressionThreshold","initialChemokineExpressionValue","maxChemokineExpressionValue","maxProbabilityOfAdhesion","adhesionFactorExpressionSlope")
# Output measures
measures<-c("Velocity","Displacement","PatchArea")
# Mins and max values used in sampling
sampleMaxes <- cbind(0,0.9,0.5,0.08,1,5)
sampleMins <-cbind(100,0.1,0.1,0.015,0.1,0.25)
## Partition the dataset, in this case normalising the data
partitionedData <- partition_dataset(sim_data_for_emulation,percent_train=75, percent_test=15,
percent_validation=10, seed=NULL, normalise=TRUE, parameters, sampleMins,
sampleMaxes)
devtools::load_all(".")
data("sim_data_for_emulation")
# Simulation parameters
parameters<-c("stableBindProbability","chemokineExpressionThreshold","initialChemokineExpressionValue","maxChemokineExpressionValue","maxProbabilityOfAdhesion","adhesionFactorExpressionSlope")
# Output measures
measures<-c("Velocity","Displacement","PatchArea")
# Mins and max values used in sampling
sampleMaxes <- cbind(0,0.9,0.5,0.08,1,5)
sampleMins <-cbind(100,0.1,0.1,0.015,0.1,0.25)
## Partition the dataset, in this case normalising the data
partitionedData <- partition_dataset(sim_data_for_emulation,percent_train=75, percent_test=15,
percent_validation=10, seed=NULL, normalise=TRUE, parameters, sampleMins,
sampleMaxes)
visualise_data_distribution(partitionedData$training,"Velocity","Velocity_Diagnostic")
visualise_data_distribution(partitionedData$training,"Velocity","Velocity_Diagnostic")
devtools::load_all(".")
visualise_data_distribution(partitionedData$training,"Velocity","Velocity_Diagnostic")
emulated_lhc_values <- read.csv("/home/kja505/Dropbox/IEEE_AIS/Generated_Emulations/Spartan_Tutorial/72/LHC_Parameters_for_Runs.csv",header=T)
devtools::use_data(emulated_lhc_values)
head(emulated_lhc_values)
typeof(emulated_lhc_values)
head(sim_data_for_emulation)
rm(list=ls(0))
rm(list=ls())
devtools::load_all(".")
data("sim_data_for_emulation")
# Simulation parameters
parameters<-c("stableBindProbability","chemokineExpressionThreshold","initialChemokineExpressionValue","maxChemokineExpressionValue","maxProbabilityOfAdhesion","adhesionFactorExpressionSlope")
# Output measures
measures<-c("Velocity","Displacement","PatchArea")
# Mins and max values used in sampling
sampleMaxes <- cbind(0,0.9,0.5,0.08,1,5)
sampleMins <-cbind(100,0.1,0.1,0.015,0.1,0.25)
## Partition the dataset, in this case normalising the data
partitionedData <- partition_dataset(sim_data_for_emulation,percent_train=75, percent_test=15,
percent_validation=10, seed=NULL, normalise=TRUE, parameters, sampleMins,
sampleMaxes)
visualise_data_distribution(partitionedData$training,"Velocity","Velocity_Diagnostic")
# If you wish, you can specify the number of bins to use in the plot. A default of 30 is used if not specified
visualise_data_distribution(partitionedData$training,"Displacement","Displacement_Diagnostic", num_bins = 10)
networkStructures<-list(c(4),c(3),c(4,3),c(4,4,3),c(4,4),c(4,3,3),c(4,4,4,3),c(4,3,2))
networkStructures<-list(c(4),c(3),c(4,3),c(4,4,3),c(4,4),c(4,3,3),c(4,4,4,3),c(4,3,2))
# So change the default in the algorithm settings
algorithmSettings<-emulation_algorithm_settings(network_structures=networkStructures)
glm_fit <- generate_requested_emulations(c("GLM"),partitionedData, parameters,measures, normalised=TRUE)
svm_fit <- generate_requested_emulations(c("SVM"),partitionedData, parameters,measures, normalised=TRUE)
rf_fit <- generate_requested_emulations(c("RF"),partitionedData, parameters,measures, normalised=TRUE)
nn_fit <- generate_requested_emulations(c("NNET"),partitionedData, parameters,measures,algorithmSettings, normalised=TRUE)
gp_fit <- generate_requested_emulations(c("GP"),partitionedData, parameters,measures, normalised=TRUE)
predictions <- emulator_predictions(gp_fit, parameters, measures,	partitionedData$validation, normalise=FALSE)
predictions <- emulator_predictions(rf_fit, parameters, measures,	partitionedData$validation, normalise=FALSE)
head(predictions)
devtools::load_all(".")
predictions <- emulator_predictions(rf_fit, parameters, measures,	partitionedData$validation, normalise=FALSE, normalise_result= TRUE)
head(predictions)
data("emulated_lhc_values")
ensemble <- nn_fit
parameters <- c("stableBindProbability","chemokineExpressionThreshold",
"initialChemokineExpressionValue",
"maxChemokineExpressionValue","maxProbabilityOfAdhesion",
"adhesionFactorExpressionSlope")
# Declare responses:
measures<-c("Velocity","Displacement")
measure_scale <- c("microns","microns/min")
# Spartan file name for LHC
parameterFileName <- "LHC_Parameters_for_Runs.csv"
# Now generate predictions:
emulate_lhc_sampled_parameters(filepath, parameterFileName, ensemble, parameters, measures, measure_scale)
filepath<-getwd()
# Declare responses:
measures<-c("Velocity","Displacement")
measure_scale <- c("microns","microns/min")
# Spartan file name for LHC
parameterFileName <- "LHC_Parameters_for_Runs.csv"
# Now generate predictions:
emulate_lhc_sampled_parameters(filepath, parameterFileName, ensemble, parameters, measures, measure_scale)
emulate_lhc_sampled_parameters(filepath, dataset=emulated_lhc_values, ensemble, parameters, measures, measure_scale)
devtools::load_all(".")
emulate_lhc_sampled_parameters(filepath, dataset=emulated_lhc_values, ensemble, parameters, measures, measure_scale)
devtools::load_all(".")
emulate_lhc_sampled_parameters(filepath, dataset=emulated_lhc_values, ensemble, parameters, measures, measure_scale)
devtools::load_all(".")
emulate_lhc_sampled_parameters(filepath, dataset=emulated_lhc_values, ensemble, parameters, measures, measure_scale)
devtools::load_all(".")
emulate_lhc_sampled_parameters(filepath, ensemble, parameters, measures, measure_scale, dataset = emulated_lhc_values)
devtools::load_all(".")
emulate_lhc_sampled_parameters(filepath, ensemble, parameters, measures, measure_scale, dataset = emulated_lhc_values)
is.null(emulated_lhc_values)
devtools::load_all(".")
emulate_lhc_sampled_parameters(filepath, ensemble, parameters, measures, measure_scale, dataset = emulated_lhc_values)
devtools::load_all(".")
emulate_lhc_sampled_parameters(filepath, ensemble, parameters, measures, measure_scale, dataset = emulated_lhc_values)
filepath <- getwd()
# Declare parameters
parameters <- c("stableBindProbability","chemokineExpressionThreshold",
"initialChemokineExpressionValue",
"maxChemokineExpressionValue","maxProbabilityOfAdhesion",
"adhesionFactorExpressionSlope")
numSamples <- 500
# Mins and Maxes
pmaxes <- cbind(0,0.9,0.5,0.08,1,5)
pmins <-cbind(100,0.1,0.1,0.015,0.1,0.25)
data("emulated_lhc_values")
ensemble<-nn_fit
measures<-c("Velocity","Displacement")
measure_scale <- c("microns","microns/min")
emulate_lhc_sampled_parameters(filepath, ensemble, parameters, measures, measure_scale, dataset = emulated_lhc_values)
devtools::load_all(".")
emulate_lhc_sampled_parameters(filepath, ensemble, parameters, measures, measure_scale, dataset = emulated_lhc_values)
param_file = NULL
dataset <- emulated_lhc_values
!is.null(param_file) & !is.null(dataset)
is.null(param_file)
!is.null(param_file) | !is.null(dataset)
devtools::load_all(".")
emulate_lhc_sampled_parameters(filepath, ensemble, parameters, measures, measure_scale, dataset = emulated_lhc_values)
devtools::load_all(".")
emulate_lhc_sampled_parameters(filepath, ensemble, parameters, measures, measure_scale, dataset = emulated_lhc_values)
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
emulate_lhc_sampled_parameters(filepath, ensemble, parameters, measures, measure_scale, dataset = emulated_lhc_values)
surrogateModel <- nn_fit
emulate_lhc_sampled_parameters(filepath, surrogateModel, parameters, measures, measure_scale, dataset = emulated_lhc_values, ensemble=FALSE)
devtools::load_all(".")
devtools::document()
descArgs <- list(Package = "spartan",
Title = "Simulation Parameter Analysis R Toolkit ApplicatioN: Spartan",
Description = "Computer simulations are becoming a popular technique to use in attempts to further our understanding of complex systems. SPARTAN, first described in our 2013 publication in PLoS Computational Biology, provided code for four techniques described in available literature which aid the analysis of simulation results, at both single and multiple timepoints in the simulation run. The first technique addresses aleatory uncertainty in the system caused through inherent stochasticity, and determines the number of replicate runs necessary to generate a representative result. The second examines how robust a simulation is to parameter perturbation, through the use of a one-at-a-time parameter analysis technique. Thirdly, a latin hypercube based sensitivity analysis technique is included which can elucidate non-linear effects between parameters and indicate implications of epistemic uncertainty with reference to the system being modelled. Finally, a further sensitivity analysis technique, the extended Fourier Amplitude Sampling Test (eFAST) has been included to partition the variance in simulation results between input parameters, to determine the parameters which have a significant effect on simulation behaviour. Version 1.3 added support for Netlogo simulations, aiding simulation developers who use Netlogo to build their simulations perform the same analyses. Version 2.0 added the ability to read all simulations in from a single CSV file in addition to the prescribed folder structure in previous versions. Version 3.0 offers significant additional functionality that permits the creation of emulations of simulation results, derived using the same sampling techniques in the global sensitivity analysis techniques, and the generation of combinations of these machine learning algorithms to one create one predictive tool, more commonly known as an ensemble model. Version 3.0 also improved the standard of the graphs produced in the original sensitivity analysis techniques, and introduced a polar plot to examine parameter sensitivity.")
options(devtools.desc = descArgs)
devtools::document()
vignette(spartan)
vignette(package=spartan)
devtools::build()
devtools::build()
devtools::build()
file.exists("~/.ssh/id_rsa.pub")
